<analysis>

**original_problem_statement:**
The user wants to build a full-stack application, Cerebro Visas, to manage and analyze US immigration visa cases. The initial session focused on setting up core features.

In this session, the user's primary goals were:
1.  Fix a critical bug preventing the creation of new cases via file import ( column missing).
2.  Implement a robust method for uploading a large number of case files at once, as the Google Drive import was unreliable. This evolved into requests for ZIP upload, folder upload, and background processing.
3.  Add the ability to edit case details after creation.
4.  Enhance the case analysis feature to be dynamic based on the visa type (specifically EB2-NIW and EB1A) and to properly consider RFE documents.
5.  Fix text extraction (OCR) for scanned PDF documents, which was failing.

PRODUCT REQUIREMENTS:
- **Capa A - Ingesta y normalización:** The user needs a reliable bulk upload mechanism that can handle hundreds of files, either via folder selection or ZIP files. The system must extract text from both digital and scanned PDFs.
- **Capa B - Extracción estructurada:** An LLM Case Miner must dynamically apply the correct visa taxonomy (EB2-NIW vs. EB1A) to extract structured data from documents like RFEs.
- **Capa C - Memoria dual:** The database must store the full extracted text of documents.
- **Capa E - Chat + Auditor:** The Analizar Caso feature must be intelligent, using the correct legal criteria for the specific visa type and prioritizing RFE documents in its analysis. All LLM-dependent features require a valid API key to function.

**User's preferred language**: Spanish (español). The next agent MUST respond in Spanish.

**what currently exists?**
The application's file ingestion capabilities have been significantly reworked and enhanced. The  page now serves as a central hub for uploads with multiple options:
- **Tab Archivos:** A robust multi-file uploader with drag-and-drop support for both files and folders. It correctly handles adding files in batches and avoids duplicates.
- **Tab ZIP Grande:** A new system for background processing of large ZIP files. It uploads the file to Supabase Storage and creates a job in the  table. **However, this is currently non-functional for large files due to a Supabase Storage browser-upload limit.**
- **Tab En Proceso:** A new UI to monitor the status of background import jobs.
- **Add to Existing Case:** The import page now allows the user to select an existing case from a dropdown and add new files to it, preventing duplicates.

Additionally, key features have been added or improved:
- **Edit Case Data:** The  page now has an Editar button that opens a modal to update client and visa information. The backend  API is confirmed to be working.
- **Dynamic Case Analysis:** The analysis engine () and the Case Miner () have been upgraded. They now dynamically detect the visa category (EB2-NIW vs. EB1A) and apply the correct taxonomy and analysis criteria. This was tested and works, but is currently broken due to the API key issue.
- The initial  vs  database schema mismatch has been fixed across all relevant API endpoints and frontend pages.

**Last working item**:
-   **Last item agent was working:** Diagnosing and attempting to fix a critical failure in all LLM-powered features (Case Analysis, OCR for scanned PDFs).
-   **Status:** BLOCKED
-   **Reasoning:** The agent correctly identified the root cause: the  stored in the  file is invalid or has expired, resulting in a  error on every LLM call. The agent attempted to use a system-provided key, which failed, and has correctly concluded that it is blocked waiting for a new, valid key from the user.
-   **Agent Testing Done:** Y (The agent confirmed the API key is invalid by making direct  calls).
-   **Which testing method agent to use?** N/A. The agent is blocked waiting for user input (a valid API key).
-   **User Testing Done:** N

**All Pending/In progress Issue list**:
-   Issue 1: Invalid LLM API Key is breaking core features (P0 - CRITICAL BLOCKER)
-   Issue 2: ZIP Grande background upload is non-functional for large files (P1)
-   Issue 3: Drag-and-drop for folders has a file limit (P2)
-   Issue 4: SQL migration for  needs user confirmation (P2)

**Issues Detail:**
-   **Issue 1: Invalid LLM API Key is breaking core features**
    -   **Description**: All features that rely on an LLM, including Case Analysis () and OCR for scanned documents (), are failing with a  error. The  in  is invalid.
    -   **Attempted fixes**: The agent tried using an internal  but the integration was incorrect. It correctly reverted this change.
    -   **Next debug checklist**:
        1.  Ask the user to provide a new, valid API key from OpenRouter, OpenAI, or Google AI.
        2.  Use the  tool to update the  in the  file.
        3.  Restart the  service using nextjs: ERROR (not running)
nextjs: ERROR (abnormal termination).
        4.  Test a failing feature, for example, by re-uploading a scanned PDF and checking if  is populated, or by running the Analizar Caso feature.
    -   **Why fix this issue and what will be achieved with the fix?**: This is the most critical issue. Fixing it will unblock the core value proposition of the application: AI-powered case analysis and data extraction.
    -   **Status**: BLOCKED
    -   **Is recurring issue?**: Y (Throughout the last part of the session).
    -   **Should Test frontend/backend/both after fix?**: backend
    -   **Blocked on other issue**: Blocked on user providing a valid API key.

-   **Issue 2: ZIP Grande background upload is non-functional for large files**
    -   **Description**: The agent built a feature to upload large ZIPs to Supabase Storage for background processing. However, the upload gets stuck at 0% because the Supabase browser client library has a file size limit (around 50MB on the free plan), which is much smaller than the user's 1.7GB file. The architecture is sound, but the implementation is incompatible with the environment's limitations.
    -   **Attempted fixes**: None. The agent pivoted to suggesting alternative upload methods.
    -   **Next debug checklist**:
        1.  Inform the user about the storage limitation.
        2.  The immediate workaround is to use the Seleccionar Carpeta or Archivos tabs, which the agent already built and work well.
        3.  A long-term fix would require changing the upload mechanism to not use the browser directly, e.g., using a pre-signed URL and a server-side proxy, but this is a significant task.
    -   **Why fix this issue and what will be achieved with the fix?**: The current UI promises a feature that is broken. Fixing it would provide the user with their desired workflow for large cases. For now, guiding them to the working alternatives is key.
    -   **Status**: NOT STARTED
    -   **Is recurring issue?**: N
    -   **Should Test frontend/backend/both after fix?**: both
    -   **Blocked on other issue**: None.

-   **Issue 3: Drag-and-drop for folders has a file limit**
    -   **Description**: The user reported that when dragging a large folder, the process gets stuck at 128 files. The agent correctly identified this as a browser limitation on reading files from a dropped directory in a single batch.
    -   **Attempted fixes**: The agent implemented a workaround to allow the user to drag the same folder multiple times to add the remaining files, with duplicate detection. The better solution, which the agent also built, is the Seleccionar Carpeta button, which does not have this limitation.
    -   **Next debug checklist**:
        1.  Reinforce to the user that for large folders, the **Seleccionar Carpeta** button is the recommended and most reliable method.
    -   **Why fix this issue and what will be achieved with the fix?**: This is a minor UX issue, as a robust workaround already exists on the page.
    -   **Status**: RESOLVED (with a workaround).
    -   **Is recurring issue?**: N
    -   **Should Test frontend/backend/both after fix?**: frontend
    -   **Blocked on other issue**: None.

-   **Issue 4: SQL migration for  needs user confirmation**
    -   **Description**: The agent created  for the background processing feature. The agent provided the SQL to the user and the user reported an error from copy-pasting an incomplete snippet. The agent then provided the full, correct SQL. It's unclear if the user successfully ran it.
    -   **Attempted fixes**: Provided the correct SQL script.
    -   **Next debug checklist**:
        1.  Ask the user to confirm if they successfully ran the SQL for the  table.
        2.  If not, instruct them to run it again from the provided script.
    -   **Why fix this issue and what will be achieved with the fix?**: The background import feature (even with its current limitations) depends on this table existing.
    -   **Status**: USER VERIFICATION PENDING
    -   **Is recurring issue?**: N
    -   **Should Test frontend/backend/both after fix?**: backend
    -   **Blocked on other issue**: None.

**In progress Task List**:
None. The agent is currently blocked waiting for a valid API key.

**Upcoming and Future Tasks**
- **(P1) Fix Large File Uploads:** Re-architect the ZIP Grande feature to work around the 50MB browser upload limit, possibly by using pre-signed URLs with a server-side component.
- **(P1) Add Visual Graph to Claim Graph**: Enhance the  page with an interactive graph visualization.
- **(P2) Export Auditor Report to PDF**: Add a feature to export the Auditor de Expediente report to a PDF.
- **(P2) Implement Cohort Analyzer by Industry**: Add an  field to cases and modify the Cohort Analyzer.

**Completed work in this session**
- **Fixed  Schema Mismatch:** Corrected the column name from  to  in all relevant APIs (, ) and frontend pages, resolving the initial critical bug.
- **Implemented Edit Case Feature:** Added a modal and  API functionality to allow users to edit case details on the  page.
- **Implemented Dynamic Case Analysis:** Upgraded the analysis engine to be visa-aware, correctly applying EB2-NIW (3 prongs) or EB1A (10 criteria) taxonomies and logic based on the case's .
- **Enhanced File Import Page ():**
    - Added a tab-based UI for different upload methods.
    - Implemented a robust multi-file uploader via the Archivos tab.
    - Added support for folder uploads via a Seleccionar Carpeta button.
    - Implemented drag-and-drop for both files and folders.
    - Added logic to detect and skip duplicate files.
- **Implemented Add to Existing Case:** Added a dropdown on the import page to select an existing case and append new documents to it.
- **Implemented Background Job System (Foundation):**
    - Created the  table schema ().
    - Created backend APIs () to manage and monitor jobs.
    - Created a Supabase Storage bucket named .
    - Added UI tabs (ZIP Grande, En Proceso) to manage this workflow. (Note: The feature itself is blocked by storage limits).
- **Improved OCR Error Handling:** Added a size validation (300MB) to the synchronous ZIP upload API to prevent server crashes from oversized files.

**Code Architecture**


**Key Technical Concepts**
- **Framework:** Next.js 14 (App Router)
- **Database:** Supabase (PostgreSQL)
- **AI/LLM:** OpenRouter (currently requires a valid user-provided key). Models used:  for analysis,  for OCR.
- **File Uploads:**
    - Direct multi-file/folder upload via .
    - Background job pattern for large files: Direct-to-Storage upload (browser SDK) -> DB job record -> async processing.
- **Storage:** Supabase Storage for housing uploaded documents and large ZIPs.

**key DB schema**
- : (NEW) { id, client_name, status, storage_path, case_id, total_files, processed_files, ... }

**All files of reference**
-   : **CRITICAL**. The  is invalid. It needs to be replaced with a valid key from the user.
-   : The central hub for all file uploads. Contains the logic for the different tabs, adding to existing cases, and duplicate detection.
-   : The new SQL migration file for the background jobs feature. The user must run this.
-   : The OCR logic, currently failing due to the invalid API key.
-   : The case analysis API, also failing due to the invalid API key.

**Critical Info for New Agent**
-   **IMMEDIATE BLOCKER**: You **MUST** obtain a new, valid API key from the user for a service like OpenRouter, OpenAI, or Google AI. All AI-powered features (case analysis, OCR, etc.) are currently broken. Update the  in  and restart the server.
-   **FEATURE BUG**: The ZIP Grande upload feature is **non-functional for files over ~50MB** due to a Supabase Storage limitation. Do not attempt to debug it. Advise the user to use the **Seleccionar Carpeta** or **Archivos** tabs, which are working and robust alternatives for bulk uploads.
-   **SQL MIGRATION PENDING**: You need to confirm with the user that they have executed the SQL script in  to create the  table.
-   **USER WORKFLOW**: The user's primary need is a reliable way to upload hundreds of files for a case. The Seleccionar Carpeta option is the best-fit solution currently implemented. Guide them to use it.

**Last 10 User Messages and any pending HUMAN messages**
10. Reports an error when clicking Analizar Caso. (Identified as API Key issue)
9.  Provides screenshot of the error. (Confirmed API Key issue)
8.  Asks for an explanation of the case analysis process step-by-step. (Agent provided a detailed breakdown)
7.  Asks if text content is extracted from images during upload. (Agent confirmed it should, but found the OCR is broken due to the API key)
6.  Reports an issue where drag-and-drop of a folder gets stuck at 128 files. (Agent explained the browser limit and provided workarounds)
5.  Asks if it's possible to re-upload a folder and have the system skip already-uploaded files. (Agent implemented this feature)
4.  Provides a detailed voice message explaining the need to add more files to an *existing* case after the initial 128-file import. (Agent implemented this Add to Existing Case feature)
3.  Asks to proceed with implementing background processing for large ZIP files. (Agent implemented the foundation, but it's blocked by storage limits)
2.  Reports that the drag-and-drop for folders is not working. (Agent implemented this)
1.  Asks what actualizando contenido means. (Agent was unable to find this exact text)

**Project Health Check:**
-   **Broken**:
    -   **Case Analysis:** Failing due to invalid API key.
    -   **OCR / Text Extraction from Scanned PDFs:** Failing due to invalid API key.
    -   **ZIP Grande background import:** Fails for files > 50MB due to Supabase Storage limits.
-   **Mocked**: None.

**3rd Party Integrations**
-   **Supabase**: For database, auth, storage. Requires user-provided keys.
-   **OpenRouter**: For all LLM and embedding model access. **Currently configured with an invalid API Key, which is the main blocker.** Requires a valid user-provided key.
-   **Google Drive API**: Still present but not the focus. The UI now recommends local uploads.
-   **adm-zip**: For handling .zip files on the backend.

**Testing status**
-   Testing agent used after significant changes: NO
-   Troubleshoot agent used after agent stuck in loop: NO
-   Test files created: None.
-   Known regressions: None. Core features were either fixed or newly (but correctly) implemented, aside from the API key dependency.

**Credentials to test flow:**
-   A valid API key for OpenRouter, OpenAI, or Google AI is **REQUIRED** to test any AI-related functionality.

**What agent forgot to execute**
-   The agent did not forget to execute anything. It correctly identified the root cause of the current problem (invalid API key) and is now blocked, waiting for the user to provide a valid key. It created a feature (ZIP Grande) that is architecturally incompatible with the platform's free tier limits, which should be communicated to the user.

</analysis>
